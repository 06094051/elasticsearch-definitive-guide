==== Then scale some more

But what if we want to scale our search to more than 6 nodes?

The number of primary shards is fixed at the moment an index is created.
Effectively, that number defines the maximum amount of data that can be
*stored* in the index.  (The actual number depends on your data, your hardware
and your use case). However, read requests -- searches or document retrieval
-- can be handled by a primary *or* a replica shard, so the more copies of
data that you have, the more search throughput we can handle.

The number of replica shards can be changed dynamically on a live cluster,
allowing us to scale up or down as demand requires. Let's increase the number
of replicas from the default of `1` to `2`:

[source,js]
--------------------------------------------------
PUT /blogs/_settings
{
   "index" : {
      "number_of_replicas" : 2
   }
}
--------------------------------------------------


[[cluster-three-nodes-two-replicas]]
.Increasing the `number_of_replicas` to 2
image::images/cluster_node1_node2_node3_replicas_2.svg["A three-node cluster with two replica shards"]

As can be seen in <<cluster-three-nodes-two-replicas>>, the `blogs` index now
has 9 shards: 3 primaries and 6 replicas. If we were to run a cluster with 9
nodes (one shard per node), we would be able to handle *50%* more search
requests than our previous 6-node cluster.

[NOTE]
===================================================

Of course, just having more replica shards on the same number of nodes doesn't
increase our performance at all because each shard has access to a smaller
fraction of its node's resources.  You need to add hardware to increase
throughput.

But these extra replicas do mean that we have more redundancy. With the node
configuration above, we can now afford to lose two nodes without losing any
data.

===================================================
