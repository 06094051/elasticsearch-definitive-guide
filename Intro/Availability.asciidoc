=== Always available

Hardware fails, and the more hardware you have, the more likely failure is.
Elasticsearch uses multiple nodes to ensure that the service and data is
always available, even when individual nodes fail.


==== An empty cluster

If we start a single node, with no data, our cluster looks like <<img-cluster>>.

[[img-cluster]]
.A cluster with one empty node
image::images/cluster.svg["A cluster with one empty node"]

==== Add an index

Now let's create an index called `blogs` in our empty one-node cluster.
By default, indices are assigned 5 primary shards, but for the purpose of this
demonstration, we'll assign just 3 primary shards:

    curl -XPUT 'http://127.0.0.1:9200/blogs/?pretty'  -d '
    {
       "settings" : {
          "number_of_shards" : 3
       }
    }
    '

Our cluster now looks like <<cluster-one-node>> -- all 3 primary shards have
been assigned to `Node 1`.

[[cluster-one-node]]
.A single-node cluster with an index
image::images/cluster_node1.svg["A single-node cluster with an index"]

You'll notice that there is no single box representing an index -- the index
is just a logical namespace which groups the three shards together.

==== Add failover

Running a single node means that you have a single point of failure -- there
is no redundancy. However, if we start a second node, our cluster now looks like
<<cluster-two-nodes>>.

[[cluster-two-nodes]]
.A two-node cluster -- all primary and replica shards are allocated
image::images/cluster_node1_node2.svg["A two-node cluster"]

The second node has joined the cluster and created a copy or _replica shard_ of
each primary shard.  That means that we can lose either node and all of our
data will be intact.

Any document that is added to our index is first indexed to the primary
shard and then in parallel to any replica shards, ensuring that our
document is immediately available from any node in the cluster.

==== Scale horizontally

Now our cluster is _always available_, but what about scaling as the demand
for our application grows? If we start a third node, our cluster reorganizes
itself to look like <<cluster-three-nodes>>.

[[cluster-three-nodes]]
.A three-node cluster -- shards have been reallocated to spread the load
image::images/cluster_node1_node2_node3.svg["A three-node cluster"]

One shard each from `Node 1` and `Node 2` have moved to the new
`Node 3`, meaning that we now have two shards per node, instead of three.
This means that the hardware resources (CPU, RAM, I/O) of each node
are being shared between fewer shards, allowing each shard to perform
better.

A shard is a fully fledged search engine in its own right, and is
capable of using all of the resources of a single node.  With our
total of 6 shards (3 primaries and 3 replicas) our index can span
a maximum of 6 nodes, each with one shard.

==== Then scale some more

But what if we want to scale our search to more than 6 nodes?

The number of primary shards is fixed at the moment an index is created.
Effectively, that number defines the limit of the total amount of data that
can be stored in the index.  (The actual limit depends on your data, your
hardware and your use case).

However, searches or document retrieval can be handled by a primary or
a replica shard, so the more copies of data that have, the more
search throughput we can handle.

We can change the number of replica shards dynamically, on a live cluster,
allowing us to scale up or down as demand requires.

Let's increase the number of replicas from the default of `1` to `2`:

    curl -XPUT 'http://127.0.0.1:9200/blogs/_settings?pretty'  -d '
    {
       "index" : {
          "number_of_replicas" : 2
       }
    }
    '

As can be seen in <<cluster-three-nodes-two-replicas>>, the `blogs` index
now has 9 shards: 3 primaries and 6 replicas. A 9-node cluster could now
handle 50% more search requests than a 6-node cluster.

[[cluster-three-nodes-two-replicas]]
.Increasing the `number_of_replicas` to 2
image::images/cluster_node1_node2_node3_replicas_2.svg["A three-node cluster with two replica shards"]

Of course, just having more replica shards on the same number of nodes doesn't
increase our performance, but it does mean that we have more redundancy.
We can now afford to lose two nodes without losing any data.

==== Coping with failure

KILL NODES
