
== Approximate Aggregations

Life is easy if all of your data fits on a single machine.  Classic algorithms
taught in CS201 will be sufficient for all your needs.  But if all your data fits
on a single machine, there would be no need for distributed software
like Elasticsearch at all.  Once you start distributing data, you need to start
evaluating algorithms and potential trade-offs.

Many algorithms are amenable to distributed execution.  All of the aggregations
discussed thus far execute in a single-pass, and give exact results. These types 
of algorithms are often referred to as "embarrassingly parallel", 
because they parallelize to multiple machines with little effort.  When 
performing a `max` metric, the underlying algorithm is very simple:

1. Broadcast the request to all shards
2. Look at the "price" field for each document.  If `price > current_max`, replace
`current_max` with `price`
3. Return maximum price from all shards to coordinating node
4. Find maximum price returned from all shards.  This is the true maximum.

The algorithm scales linearly with machines because the algorithm requires no
coordination (the machines don't need to discuss intermediate results), and the 
memory footprint is very small (a single integer representing the maximum).

There are some operations which we would like to perform, however, which are
_not_ embarrassingly parallel.  For these algorithms, you need to
start making trade-offs.  There is a triangle of factors at play: "big data",
exactness, real-time latency.

You get to choose two from this triangle.

- Exact + Real-time: Your data fits in the RAM of a single machine.  The world
is your oyster, use any algorithm you want

- Big Data + Exact:  A classic hadoop installation.  Can handle petabytes of data
and give you exact answers...but it may take a week to give you that answer

- Big Data + Real-time: Approximate algorithms which give you accurate, but not
exact, results

Elasticsearch currently supports two approximate algorithms (`cardinality` and 
`percentiles`).  These will give you accurate results but not 100% exact.
These algorithms trade exactness for a small memory footprint and/or faster
execution speed.

For _most_ domains, highly accurate results that return _in real time_ across
_all your data_ is more important than 100% exactness.  When trying to determine
what the 99th percentile of latency is for your website, you often don't care if
that value is off by 0.1%. 

But you do care if it takes five minutes to load the dashboard.  This is an example
where a little estimation gives you huge advantages with minimal downside.




