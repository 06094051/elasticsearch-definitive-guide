
=== Field Data Filtering

Now that we've seen how field data is stored in memory, and how to limit that
memory usage, we are going to talk about ways to reduce the memory burden of
field data.  There are several techniques, each with their own benefits and
downsides.

The first that we will talk about is field data _filtering_.  When field data is
being loaded, the data can be passed through one or more pre-configured filters.
These filter limit what is loaded to memory, and the memory savings are directly 
proportional to how strict your filters are.

Why might filtering be useful?  Imagine you only run `terms` aggregations, which
are used to calculate the "top-ten tags" for your website.  You may have
thousands of tags, but you only display the top ten.  Most data follows
a Zipfian distribution and might look something like this:

TODO: image of zipf distribution

A handful of tags have large counts, but the vast majority are very infrequent.
These tags are guaranteed to never show up in your top-ten -- they are loaded
to memory for no real purpose. Additionally, all of these values add time to the 
aggregation execution, meaning the entire query takes longer than it needs to.

If you have this scenario -- a long tail of data that is never used -- you might
consider enabling frequency-based filtering.  This is enabled in the mappings
of the field:

[source,js]
----
PUT /fielddata/filtering/_mapping
{
    "filtering" : {
        "properties" : {
            "tag": {
                "type":      "string",
                "fielddata": {  <1>
                    "filter": {
                        "frequency": {  <2>
                            "min":              0.01, <3>
                            "max":              0.4, <4>
                            "min_segment_size": 500
                        }
                    }
                }
            }
        }
    }
}
----
<1> A "fielddata" parameter is added to the properties of the "tag" field
<2> A `frequency` filter is specified, which automatically filters based on term
frequency
<3> 0.01 == 1% doc frequency
<4> 0.4 == 40% doc frequency

This mapping instructs Elasticsearch to filter the "tag" field when it is first
loaded into field data.  It works based on document frequency (percentage of
documents containing the field, which have a particular term).  In the example, 
terms that are found in less than 1% of the documents will not be loaded at all.
Similarly, terms that are more frequent than 40% (common words like "the") will 
also be excluded.

Finally, there is a `min_segment_size`, which skips segments which have fewer than
500 docs.  This is usually important since small segments don't have enough documents
to follow Zipf's law.  Filtering small segments might skew results and so we just
skip them completely.

All three parameters are optional, so mix and match as appropriate for your 
requirements.

Field data filtering can have *massive* impacts on memory usage.  The trade-off is
fairly obvious: you are essentially ignoring data.  But for many applications,
the trade-off is reasonable since the data is not being used anyway.  The memory
savings is often more important than including a large and relatively useless
long tail of terms.
