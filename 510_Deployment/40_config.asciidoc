[[重要配置的修改]]

=== 重要配置的修改
Elasticsearch 已经有了 _很好_ 的默认值，((("deployment", "configuration changes, important")))((("configuration changes, important")))特别是涉及到性能相关的配置或者选项。
如果你有疑问，最好就不要动它。我们已经目睹了数十个因为错误的设置而导致毁灭的集群，
因为它的管理者总认为改动一个配置或者选项就可以带来100倍的提升。

[NOTE]
====
请阅读整节文章，所有的配置项都同等重要，和描述顺序无关，请阅读所有的配置选项，并应用到你的集群中。
====

其它数据库可能需要调优，但总得来说，Elasticsearch 不需要。
如果你遇到了性能问题，最好的解决方法通常是更好的数据布局或者更多的节点。
在 Elasticsearch 中很少有“神奇的配置项”，
如果存在，我们也已经帮你优化了。

说到这里，有一些保障性的配置需要在生产环境中做修改。
这些改动是必须的，因为没有办法设定好的默认值（它取决于你的集群布局）。


==== 指定名字

Elasticsearch 默认启动的集群名字叫 `elasticsearch`。((("configuration changes, important", "assigning names")))你最好
给你的生产环境的集群改个名字，改名字的目的很简单，
就是防止某个人的笔记本加入到了集群。简单修改成 `elasticsearch_production` 会很省心。

你可以在你的 `elasticsearch.yml` 文件中：

[source,yaml]
----
cluster.name: elasticsearch_production
----

同样，最好也修改你的节点名字。就像你现在可能发现的那样，
Elasticsearch 会在你的节点启动的时候随机给它指定一个名字。你可能会觉得这很有趣，但是当凌晨3点钟的时候，
你还在尝试回忆哪台物理机是 `Tagak the Leopard Lord` 的时候，你就不觉得有趣了。

更重要的是，这些名字是在启动的时候产生的，每次启动节点，
它都会得到一个新的名字。这会使日志变得很混乱，因为所有节点的名称都是不断变化的。

这可能会让你觉得厌烦，我们建议给每个节点设置一个有意义的、清楚的、描述性的名字，同样你可以在 `elasticsearch.yml` 中配置：

[source,yaml]
----
node.name: elasticsearch_005_data
----


==== 路径

默认情况下，((("configuration changes, important", "paths")))((("paths")))Elasticsearch 会把插件、日志以及你最重要的数据放在安装目录下。这会带来不幸的事故，
如果你重新安装 Elasticsearch 的时候不小心把安装目录覆盖了。如果你不小心，你就可能把你的全部数据删掉了。

不要笑，这种情况，我们见过很多次了。

最好的选择就是把你的数据目录配置到安装目录以外的地方，
同样你也可以选择转移你的插件和日志目录。

可以更改如下：

[source,yaml]
----
path.data: /path/to/data1,/path/to/data2 <1>

# Path to log files:
path.logs: /path/to/logs

# Path to where plugins are installed:
path.plugins: /path/to/plugins
----
<1> 注意：你可以通过逗号分隔指定多个目录。

数据可以保存到多个不同的目录，
每个目录如果是挂载在不同的硬盘，做 RAID 0 是一个简单而有效的办法。Elasticsearch 会自动把数据分隔到不同的目录，以便提高性能。

.Multiple data path safety and performance
[WARNING]
====================
Like any RAID 0 configuration, only a single copy of your data is saved to the
hard drives.  If you lose a hard drive, you are _guaranteed_ to lose a portion
of your data on that machine.  With luck you'll have replicas elsewhere in the
cluster which can recover the data, and/or a recent <<backing-up-your-cluster, backup>>.

Elasticsearch attempts to minimize the extent of data loss by striping entire
shards to a drive.  That means that `Shard 0` will be placed entirely on a single
drive. Elasticsearch will not stripe a shard across multiple drives, since the
loss of one drive would corrupt the entire shard.

This has ramifications for performance: if you are adding multiple drives
to improve the performance of a single index, it is unlikely to help since
most nodes will only have one shard, and thus one active drive.  Multiple data
paths only helps if you have many indices/shards on a single node.

Multiple data paths is a nice convenience feature, but at the end of the day,
Elasticsearch is not a software RAID package. If you need more advanced configuration,
robustness and flexibility, we encourage you to use actual software RAID packages
instead of the multiple data path feature.
====================

==== 最小主节点数

`minimum_master_nodes` 设定对你的集群的稳定 _及其_ 重要。
((("configuration changes, important", "minimum_master_nodes setting")))((("minimum_master_nodes setting")))
当你的集群中有两个 masters（注：主节点）的时候，这个配置有助于防止集群分裂（注：脑裂）。

如果你的集群发生了一个脑裂，那么你的集群就会处在丢失数据的危险中，因为
节点是被认为是这个集群的最高统治者，它决定了什么时候新的索引可以创建，多少分片要移动等等。如果你有 _两个_ masters 节点，
你的数据的完整性将得不到保证，因为你有两个节点认为他们有集群的控制权。

这个配置就是告诉 Elasticsearch 当没有足够 master 候选节点的时候，就不要进行 master 节点选举，等 master 候选节点足够了才进行选举。

此设置应该始终被配置为 master 候选节点的法定个数（大多数个）。((("quorum")))法定个数就是 `( master 候选节点个数 / 2) + 1`。
这里有几个例子：

- 如果你有10个节点（能保存数据，同时能成为 master），法定数就是 `6`。
- 如果你有3个候选 master 节点，和100个 date 节点，法定数就是 `2`，你只要数数那些可以做 master 的节点数就可以了。
- 如果你有两个节点，你遇到难题了。法定数当然是 `2`，但是这意味着如果有一个节点挂掉，你整个集群就不可用了。
设置成 `1` 可以保证集群的功能，但是就无法保证集群脑裂了，像这样的情况，你最好至少保证有3个节点。

你可以在你的 `elasticsearch.yml` 文件中这样配置：

[source,yaml]
----
discovery.zen.minimum_master_nodes: 2
----

但是由于 ELasticsearch 是动态的，你可以很容易的添加和删除节点，
但是这会改变这个法定个数。
你不得不修改每一个索引节点的配置并且重启你的整个集群只是为了让配置生效，这将是非常痛苦的一件事情。

基于这个原因，`minimum_master_nodes`（还有一些其它配置）允许通过 API 调用的方式动态进行配置。
当你的集群在线运行的时候，你可以这样修改配置：

[source,js]
----
PUT /_cluster/settings
{
    "persistent" : {
        "discovery.zen.minimum_master_nodes" : 2
    }
}
----

这将成为一个永久的配置，并且无论你配置项里配置的如何，这个将优先生效。当你添加和删除master节点的时候，你需要更改这个配置。

==== 集群恢复方面的配置

当你集群重启时，几个配置项影响你的分片恢复的表现。((("recovery settings")))((("configuration changes, important", "recovery settings")))首先，我们需要明白
如果什么也没配置将会发生什么。

Imagine you have ten nodes, and each node holds a single shard--either a primary
or a replica--in a 5 primary / 1 replica index.  You take your
entire cluster offline for maintenance (installing new drives, for example).  When you
restart your cluster, it just so happens that five nodes come online before
the other five.

Maybe the switch to the other five is being flaky, and they didn't
receive the restart command right away.  Whatever the reason, you have five nodes
online.  These five nodes will gossip with each other, elect a master, and form a
cluster.  They notice that data is no longer evenly distributed, since five
nodes are missing from the cluster, and immediately start replicating new
shards between each other.

Finally, your other five nodes turn on and join the cluster.  These nodes see
that _their_ data is being replicated to other nodes, so they delete their local
data (since it is now redundant, and may be outdated).  Then the cluster starts
to rebalance even more, since the cluster size just went from five to ten.

During this whole process, your nodes are thrashing the disk and network, moving
data around--for no good reason. For large clusters with terabytes of data,
this useless shuffling of data can take a _really long time_.  If all the nodes
had simply waited for the cluster to come online, all the data would have been
local and nothing would need to move.

Now that we know the problem, we can configure a few settings to alleviate it.
First, we need to give Elasticsearch a hard limit:

[source,yaml]
----
gateway.recover_after_nodes: 8
----

This will prevent Elasticsearch from starting a recovery until at least eight (data or master) nodes
are present.  The value for this setting is a matter of personal preference: how
many nodes do you want present before you consider your cluster functional?
In this case, we are setting it to `8`, which means the cluster is inoperable
unless there are at least eight nodes.

Then we tell Elasticsearch how many nodes _should_ be in the cluster, and how
long we want to wait for all those nodes:

[source,yaml]
----
gateway.expected_nodes: 10
gateway.recover_after_time: 5m
----

What this means is that Elasticsearch will do the following:

- Wait for eight nodes to be present
- Begin recovering after 5 minutes _or_ after ten nodes have joined the cluster,
whichever comes first.

These three settings allow you to avoid the excessive shard swapping that can
occur on cluster restarts.  It can literally make recovery take seconds instead
of hours.

NOTE: These settings can only be set in the `config/elasticsearch.yml` file or on
the command line (they are not dynamically updatable) and they are only relevant
during a full cluster restart.

[[unicast]]
==== Prefer Unicast over Multicast

Elasticsearch is configured to use unicast discovery out of the box to prevent
nodes from accidentally joining a cluster. Only nodes running on the same
machine will automatically form cluster.

While multicast is still https://www.elastic.co/guide/en/elasticsearch/plugins/current/discovery-multicast.html[provided
as a plugin], it should never be used in production. The
last thing you want is for nodes to accidentally join your production network, simply
because they received an errant multicast ping.  There is nothing wrong with
multicast _per se_.  Multicast simply leads to silly problems, and can be a bit
more fragile (for example, a network engineer fiddles with the network without telling
you--and all of a sudden nodes can't find each other anymore).

To use unicast, you provide Elasticsearch a list of nodes that it should try to contact.
When a node contacts a member of the unicast list, it receives a full cluster
state that lists all of the nodes in the cluster.  It then contacts
the master and joins the cluster.

This means your unicast list does not need to include all of the nodes in your cluster.
It just needs enough nodes that a new node can find someone to talk to.  If you
use dedicated masters, just list your three dedicated masters and call it a day.
This setting is configured in `elasticsearch.yml`:

[source,yaml]
----
discovery.zen.ping.unicast.hosts: ["host1", "host2:port"]
----

For more information about how Elasticsearch nodes find eachother, see
https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery-zen.html[Zen Discovery]
in the Elasticsearch Reference.
