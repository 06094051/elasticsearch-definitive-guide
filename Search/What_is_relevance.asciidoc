=== What is relevance?

We've mentioned that, by default, results are returned in descending
order of relevance. But what is relevance? How is it calculated?

The relevance of each document is represented by a positive floating point
number called the `_score` -- the higher the `_score`, the more relevant
the document.

A query clause generates a `_score` for each document.  How that score is
calculated depends on the type of query clause -- different query clauses are
used for different purposes: a `fuzzy` query would determine the `_score` by
calculating how similar the spelling of the found word is to the original
search term, a `terms` query would incorporate the percentage of terms that
were found, but what we usually mean by _similarity_ is the algorithm that we
use to calculate how similar the contents of a full text field are to a full
text query string.

The standard _similarity algorithm_ used in Elasticsearch is known as TF/IDF,
or Term Frequency/Inverse Document Frequency, which takes the following
factors into account:

Term frequency::
  How often does each term appear in the field? The more often, the more
  relevant.

Inverse document frequency::
  How often does each term appear in the index?  The more often, the *less*
  relevant.

Length norm::
  How long is the field? The longer it is, the less likely it is
  that words in the field will be relevant.

Individual queries may combine the TF/IDF score with other factors
such as the term proximity in phrase queries, or term similarity in
fuzzy queries.

Relevance is not just about full text search. It can equally be
applied to `yes|no` clauses, where the more clauses that match, the higher
the `_score`.

When multiple query clauses are combined using a compound query like the
`bool` query, the `_score` from each of these query clauses is combined to
calculate the overall `_score` for the document.

==== Understanding the score

When debugging a complex query, it can be difficult to understand
exactly how a `_score` has been calculated.  Elasticsearch
has the option of producing an _explanation_ with every search result,
by setting the `explain` parameter to `true`.


[source,js]
--------------------------------------------------
GET /_search
{
   "query"   : { "match" : { "tweet" : "text" }},
   "explain" : true
}
--------------------------------------------------


****
Adding `explain` produces a lot of output for every hit which can look
overwhelming, but it is worth taking the time to understand what it all means.
Don't worry if it doesn't all make sense now -- you can refer to this section
when you need it.  We'll work through the output for one `hit` bit by bit.
****

First, we have the metadata that is returned on normal search requests:

[source,js]
--------------------------------------------------
{
    "_index" :      "us",
    "_type" :       "tweet",
    "_id" :         "5",
    "_score" :      0.13424811,
    "_source" :     { ... trimmed ... },
--------------------------------------------------


It adds information about which shard on which node the document came from,
which is useful to know because term and document frequencies are calculated
per shard, rather than per index:

[source,js]
--------------------------------------------------
    "_shard" :      1,
    "_node" :       "mzIVYCsqSWCG_M_ZffSs9Q",
--------------------------------------------------


Then it provides the `_explanation`. Each entry contains a  `description`
which tells you what type of calculation is being performed, a `value`
which gives you the result of the calculation, and the `details` of any
sub-calculations that were required.

[source,js]
--------------------------------------------------
    "_explanation" : {
        "description" :             "weight(tweet:text in 0)
                                     [PerFieldSimilarity], result of:",
        "value" :                   0.13424811,
--------------------------------------------------


This first calculation tells us that it is trying to calculate the _weight_
-- term frequency/inverse document frequency or TF/IDF --
of the term `"text"` in the field `tweet`, for document `0`.  (This is an
internal document ID and, for our purposes, can be ignored.)

As this is the uppermost calculation, its `value` will be used as the
final `_score` for the document.

The calculation consists of a few sub-calculations, the first of which
is used to calculate the inverse document frequency or `idf` --
how common is the term `"text"` in the `tweet` field of all
documents in the index:

[source,js]
--------------------------------------------------
        "details" : [
            {
                "description" :     "idf(docFreq=1, maxDocs=1)",
                "value" :           0.30685282
            },
--------------------------------------------------


Next is the `fieldNorm`, which is a measure of how long this document's `tweet`
field is -- the longer the field, the lower the `value`:

[source,js]
--------------------------------------------------
            {
                "description" :     "fieldNorm(doc=0)",
                "value" :           0.4375
            },
--------------------------------------------------


And finally, the term frequency or `tf` tells us how often the
term `"text"` appears in the `tweet` field in this document:

[source,js]
--------------------------------------------------
            {
                "value" :               0.13424811,
                "description" :         "fieldWeight in 0, product of:",
                "details" : [
                    {
                        "description" :     "tf(freq=1.0), with freq of:",
                        "value" :           1.0
                        "details" : [ {
                            "description" : "termFreq=1.0",
                            "value" :       1.0
                    }
                ]
            }
        ]
--------------------------------------------------


The output from the above can be difficult to read in JSON, but is
easier when it is formatted as YAML, by adding `format=yaml` to the
query string.


==== Understanding why a document matched

While the `explain` option adds an explanation for every result, you can
use the `explain` API to understand why a particular document matched or, more
importantly, why it *didn't* match.

The path for the request is `/index/type/id/_explain`, as in:

[source,js]
--------------------------------------------------
GET /us/tweet/3/_explain
{
   "query" : {
      "filtered" : {
         "filter" : { "term" :  { "user_id" : 1      }},
         "query" :  { "match" : { "tweet" :   "text" }}
      }
   }
}
--------------------------------------------------


Along with the full explanation that we saw above, we also now have a
`description` element, which tells us:


[source,js]
--------------------------------------------------
"failure to match filter: cache(user_id:[1 TO 1])"
--------------------------------------------------


In other words, our `user_id` filter is causing the document not to match.
