== Distributed Search Execution

Before moving on, we are going to take a detour and talk about how search is
executed in a distributed environment.  Unlike basic CRUD operations (as we 
learned in <<distributed-docs>>), search is a little more complicated.

.Content warning
****
The information presented below is for your interest. You are not
required to understand and remember all the detail in order to use Elasticsearch.
The options that are discussed are for advanced users only.

Read the section to gain a taste for how things work, and to know where
the information is in case you need to refer to it in the future,
but don't be overwhelmed by the detail.
****

Search requires a more complicated execution model because we don't know what
documents will match your query. For simple CRUD operations, we know exactly 
what document we are operating on, and more importantly, where to find it.
Using the documents index, type and ID we can immediately determine what shard
the document lives in.

With search, we have no idea what documents will match, and therefore no idea
where to start looking!  They could be inside any shard on any machine.

For this reason, search is executed in a two-phase processed called "Query then
Fetch".

=== Query Phase
The first phase is the query phase.  In this phase, your query is broadcast
to all shards in your index to determine where *candidate* documents live.
These are documents that match your query, but may or may not be returned
to the user depending on sorting and pagination.

We'll use a simple cluster setup.  We have a three-node cluster with one index,
and that index has three primary shards (but no replicas).

TODO: Add image

When a search request is sent to a node, that node becomes the coordinating node.
It is the job of this node to merge, sort and return search results to the client.
The first thing the coordinating node does is rebroadcast the query to each 
shard in the index:

Each shard now evaluates the query against the documents that reside within that
shard.  If a document matches, it is placed in a *priority queue*, which is just
a data structure that maintains the "top N documents" according to the scoring
metric.

The size of the queue is equivalent to your pagination parameters (`from` + 
`size`).  So if you query with `from: 90` and `size: 10`, each shard will build 
a priority queue that is 100 documents long.

Once each shard has finished executing the query and has built a queue of
candidate documents, it returns a list to the coordinating node.  This list is
very lightweight: it is simply the document ID and the `_score` of each doc.

On the coordinating node, we take these candidate lists and merge them together
to form one large priority queue.  Since we have three shards being
queried, and each shard constructed a queue that was 100 documents long, the 
final merged queue will be 300 documents long.

That ends the query phase and we are left with a list of documents that match
the query, sorted by their score.

.How are replicas used?
****
Our example above was simple: three nodes, three primary shards but no replicas.
If we add a replica to the index, the process is nearly identical.  Because
the replicas are an exact copy, we can treat them identical to primary shards
for querying.

This means that Elasticsearch will choose one shard from each replication group
(either the primary or one of it's replicas) and query that.  The rest of the
query and fetch process is identical.

This is why replicas can help with search performance: they spread the query 
load amongst your cluster.
****

=== Fetch Phase

After identifying which documents match, we need to actually obtain those
docs from the various shards and return them to the client.  Remember, the
output of the query phase is merely a list of documents and their score.  We
haven't extracted their source yet.  This is the job of the fetch phase.

The coordinating node first decides what documents *actually* need to be fetched.
Recall that our query only wanted documents 90-100, but our priority queue is 300
long!  The coordinating node will scan down the list to document `90` and extract
the IDs of `90` through `100`.

It will then issue 10 fetch requests to obtain these documents.  These are
essentially GET requests, and are sent back to the originating shard/nodes.
The individual shards will GET the document(s) and forward back to the coordinating node.

Once the coordinating node has obtained all 10 documents and their source, it
will "enrich" the docs with score and metadata.  Finally, the search results
will be sent back to the client.

=== Why two round-trips?
You may have noticed that the Query Then Fetch method requires two inter-cluster
round-trips.  Would it be more efficient to just send back the document after 
the query phase?  That would remove the need for an extra network round-trip.

A single round-trip method exists, called "Query And Fetch", but it
is also much slower.  Think about our example: our query wanted documents
`90` through `100`.  To accurately determine how the documents sorted, we needed
to maintain a priority queue of size 100 for *all* the shards and merge those on
the coordinating node (resulting in a final queue size of 300).

If we remove the Fetch phase, we would be forced to load all 300 documents off
disk and send those over the wire to the coordinating node.  But since our
user only wanted 10 results, we would throw away 290 documents that we 
painstakingly loaded from disk!

Networking is faster than disk access, so the penalty of an extra round-trip
is much less than loading a large number of documents only to throw them away.

Query And Fetch is used sometimes as an internal optimization.  If Elasticsearch
recognizes that only a single shard is being queried, it will 
execute everything in one pass since two phases are not needed.

=== Avoid deep pagination
The Query Then Fetch process handles pagination very well.  But deep pagination
should be avoided, since it is very expensive.  Remember that each shard must
maintain a priority queue that is `from` + `size` in length.  If you request
ten documents starting at position 10,000,000, each shard must maintain a queue
that is ten million long.  Even worse, the coordinating node must merge-sort
10m * N shards.

The overhead of simply sorting these queues quickly surpasses the query itself.
Deep pagination can easily knock a cluster over if you are not careful, from
both a CPU and memory perspective.

In practice, you don't really want or need deep pagination anyway.  Most users
become frustrated after the second page of results...rarely does anyone scroll
to page ten-thousand.  Even Google limits search results after a certain number
of pages.  

It is highly recommended to disable "infinite" paging, removing it completely
from your interface.

.Watch out for bots!
****
In addition to removing it from your interface, make sure it is limited in your
application too.  Bots have no problem adjusting URLs and paging through your
entire data set.  

Alas, Googlebot has been known to take down a cluster ro two because of 
enthusiastic pagination
****

=== Handling failure

As discussed in <<_life_inside_a_cluster>>, failure can strike your cluster.  In 
our example we have three nodes and three primary shards...but no replicas.  If
a machine were to catch on fire *right now*, you would lose some data.

Does this mean your cluster stops executing search requests until the data is
restored? Absolutely not!  It does, however, mean that your search results will
be incomplete.  

The Query Then Fetch process will continue like normal, but the coordinating 
node will make a note that one primary shard is not available for search.  
Documents will be fetched, sorted and returned to the client.  In the search
metadata you will notice that one of the shards is marked as "failed":

[source,js]
---------------------------------------------------
{
  "took" : 9,
  "timed_out" : false,
  "_shards" : {
    "total" : 3,
    "successful" : 2,
    "failed" : 1 <1>
  },
  "hits" : { ... }
}
---------------------------------------------------
<1> A shard is considered "failed" if it is unavailable or encountered an
unrecoverable error (corruption, etc)

What this means is that your cluster will continue serving requests without
interruption...but your users may potentially be missing search results until
the missing primary shards are restored.








